{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate trained models on a task and check success rate "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] [omni.isaac.kit] Interactive python shell detected but ISAAC_JUPYTER_KERNEL was not set. Problems with asyncio may occur\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import datetime\n",
    "from omniisaacgymenvs.utils.hydra_cfg.hydra_utils import *\n",
    "from omniisaacgymenvs.utils.hydra_cfg.reformat import omegaconf_to_dict, print_dict\n",
    "from rl_games.algos_torch.players import PpoPlayerDiscrete\n",
    "from rl_games.algos_torch.players import BasicPpoPlayerContinuous, BasicPpoPlayerDiscrete\n",
    "\n",
    "from omniisaacgymenvs.utils.rlgames.rlgames_utils import RLGPUAlgoObserver, RLGPUEnv\n",
    "\n",
    "from omniisaacgymenvs.scripts.rlgames_train import RLGTrainer\n",
    "from rl_games.torch_runner import Runner\n",
    "from omniisaacgymenvs.utils.task_util import initialize_task\n",
    "from omniisaacgymenvs.envs.vec_env_rlgames import VecEnvRLGames\n",
    "from omniisaacgymenvs.utils.config_utils.path_utils import retrieve_checkpoint_path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from torch._C import fork\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "\n",
    "from plot_experiment import plot_episode_data_virtual\n",
    "from eval_metrics import success_rate_from_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_multi_agents(cfg):\n",
    "\n",
    "    base_dir = \"./evaluations/\"\n",
    "    experiment_name = cfg.checkpoint.split(\"/\")[1]\n",
    "    print(f'Experiment name: {experiment_name}')\n",
    "    evaluation_dir = base_dir + experiment_name + \"/\"\n",
    "    os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "    rlg_config_dict = omegaconf_to_dict(cfg.train)\n",
    "    print(rlg_config_dict)\n",
    "    runner = Runner(RLGPUAlgoObserver())\n",
    "    runner.load(rlg_config_dict)\n",
    "    runner.reset()\n",
    "\n",
    "    agent = runner.create_player()\n",
    "    agent.restore(cfg.checkpoint)\n",
    "\n",
    "    store_all_agents = True # store all agents generated data, if false only the first agent is stored\n",
    "    is_done = False\n",
    "    env = agent.env\n",
    "    obs = env.reset()\n",
    "\n",
    "    ep_data = {'act': [], 'obs': [], 'rews': [], 'info': [], 'all_dist': []}\n",
    "    total_reward = 0\n",
    "    num_steps = 0\n",
    "    \n",
    "    total_num_steps = 800\n",
    "    for _ in range(total_num_steps):\n",
    "        actions = agent.get_action(obs['obs'], is_deterministic=True)\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        #print(f'Step {num_steps}: obs={obs[\"obs\"]}, rews={reward}, dones={done}, info={info} \\n')\n",
    "        if store_all_agents:\n",
    "            ep_data['act'].append(actions.cpu().numpy())\n",
    "            ep_data['obs'].append(obs['obs']['state'].cpu().numpy())\n",
    "            ep_data['rews'].append(reward.cpu().numpy())  \n",
    "        else:\n",
    "            ep_data['act'].append(actions[0].cpu().numpy())\n",
    "            ep_data['obs'].append(obs['obs']['state'][0].cpu().numpy())\n",
    "            ep_data['rews'].append(reward[0].cpu().numpy())\n",
    "        #ep_data['info'].append(info)\n",
    "        x_pos = obs['obs']['state'][:,6].cpu().numpy()\n",
    "        y_pos = obs['obs']['state'][:,7].cpu().numpy()\n",
    "        ep_data['all_dist'].append(np.linalg.norm(np.array([x_pos, y_pos]), axis=0))\n",
    "        total_reward += reward[0]\n",
    "        num_steps += 1\n",
    "        is_done = done.any()\n",
    "    ep_data['obs'] = np.array(ep_data['obs'])\n",
    "    ep_data['act'] = np.array(ep_data['act'])\n",
    "    ep_data['rews'] = np.array(ep_data['rews'])\n",
    "    ep_data['all_dist'] = np.array(ep_data['all_dist'])\n",
    "\n",
    "    print(f'\\n Episode: rew_sum={total_reward:.2f}, tot_steps={num_steps} \\n')\n",
    "    #print(f'Episode data: {ep_data} \\n')\n",
    "    print(f'Episode data obs shape: {ep_data[\"obs\"].shape} \\n')\n",
    "\n",
    "    #if not cfg.headless:\n",
    "    #plot_episode_data_virtual(ep_data, evaluation_dir, store_all_agents)\n",
    "    success_rate = success_rate_from_distances(ep_data['all_dist'])\n",
    "    print(success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actions_num': [2, 2, 2, 2, 2, 2, 2, 2], 'input_shape': {'masks': (8,), 'state': (10,), 'transforms': (8, 5)}, 'num_seqs': 1, 'value_size': 1, 'normalize_value': True, 'normalize_input': True, 'normalize_input_keys': ['state']}\n",
      "['state']\n",
      "True\n",
      "10\n",
      "build mlp: 10\n",
      "RunningMeanStd:  (1,)\n",
      "['state']\n",
      "here?\n",
      "RunningMeanStd:  (10,)\n",
      "=> loading checkpoint '../corl_runs/MLP_GTXY_UF_0.25_ST_PE_0.01_PAV_1.0_PLV_0.01/nn/last_MLP_GTXY_UF_0.25_ST_PE_0.01_PAV_1.0_PLV_0.01_ep_2000_rew_589.91455.pth'\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config_name = \"../cfg/train/MFP2D_PPOmulti_dict_MLP.yaml\"\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    cfg = yaml.safe_load(stream)\n",
    "\n",
    "obs_space = spaces.Dict({\"state\":spaces.Box(np.ones(10) * -np.Inf, np.ones(10) * np.Inf),\n",
    "                         \"transforms\":spaces.Box(low=-1, high=1, shape=(8, 5)),\n",
    "                         \"masks\":spaces.Box(low=0, high=1, shape=(8,))})\n",
    "\n",
    "act_space = spaces.Tuple([spaces.Discrete(2)]*8)\n",
    "\n",
    "player = BasicPpoPlayerDiscrete(cfg, obs_space, act_space, clip_actions=False, deterministic=True)\n",
    "model_path = \"../corl_runs/MLP_GTXY_UF_0.25_ST_PE_0.01_PAV_1.0_PLV_0.01/nn/last_MLP_GTXY_UF_0.25_ST_PE_0.01_PAV_1.0_PLV_0.01_ep_2000_rew_589.91455.pth\"\n",
    "player.restore(model_path)\n",
    "\n",
    "obs = dict({'state':torch.zeros((1,10), dtype=torch.float32, device='cuda'),\n",
    "            'transforms': torch.zeros(5,8, device='cuda'),\n",
    "            'masks': torch.zeros(8, dtype=torch.float32, device='cuda')})\n",
    "\n",
    "action = player.get_action(obs, is_deterministic=True)\n",
    "\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
