{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate trained models on a task and check success rate "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] [omni.isaac.kit] Interactive python shell detected but ISAAC_JUPYTER_KERNEL was not set. Problems with asyncio may occur\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import datetime\n",
    "from omniisaacgymenvs.utils.hydra_cfg.hydra_utils import *\n",
    "from omniisaacgymenvs.utils.hydra_cfg.reformat import omegaconf_to_dict, print_dict\n",
    "from rl_games.algos_torch.players import PpoPlayerDiscrete\n",
    "from rl_games.algos_torch.players import BasicPpoPlayerContinuous, BasicPpoPlayerDiscrete\n",
    "\n",
    "from omniisaacgymenvs.utils.rlgames.rlgames_utils import RLGPUAlgoObserver, RLGPUEnv\n",
    "\n",
    "from omniisaacgymenvs.scripts.rlgames_train import RLGTrainer\n",
    "from rl_games.torch_runner import Runner\n",
    "from omniisaacgymenvs.utils.task_util import initialize_task\n",
    "from omniisaacgymenvs.envs.vec_env_rlgames import VecEnvRLGames\n",
    "from omniisaacgymenvs.utils.config_utils.path_utils import retrieve_checkpoint_path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from torch._C import fork\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from plot_experiment import plot_episode_data_virtual\n",
    "from eval_metrics import success_rate_from_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments found in ../new_mass/ folder: 20\n"
     ]
    }
   ],
   "source": [
    "# specify the experiment load directory\n",
    "load_dir = \"../new_mass/\"\n",
    "experiments = os.listdir(load_dir)\n",
    "print(f'Experiments found in {load_dir} folder: {len(experiments)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out invalid experiments and retrieve valid models\n",
    "def get_valid_models(load_dir, experiment):\n",
    "    valid_models = []\n",
    "    invalid_experiments = []\n",
    "    for experiment in experiments:\n",
    "        try:\n",
    "            file_pattern = os.path.join(load_dir, experiment, \"nn\", \"last_*ep_2000_rew__*.pth\")\n",
    "            model = glob.glob(file_pattern)\n",
    "            if model:\n",
    "                valid_models.append(model[0])\n",
    "        except:\n",
    "            invalid_experiments.append(experiment)\n",
    "    if invalid_experiments:\n",
    "        print(f'Invalid experiments: {invalid_experiments}')\n",
    "    else:\n",
    "        print('All experiments are valid')\n",
    "    return valid_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All experiments are valid\n"
     ]
    }
   ],
   "source": [
    "models = get_valid_models(load_dir, experiments)\n",
    "if not models:\n",
    "    print('No valid models found')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_multi_agents(agent, models, horizon):\n",
    "\n",
    "    base_dir = \"./evaluations/\"\n",
    "    experiment_name = models[0].split(\"/\")[1]\n",
    "    print(f'Experiment name: {experiment_name}')\n",
    "    evaluation_dir = base_dir + experiment_name + \"/\"\n",
    "    os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "    agent.restore(models[0])\n",
    "\n",
    "    store_all_agents = True # store all agents generated data, if false only the first agent is stored\n",
    "    is_done = False\n",
    "    env = agent.env\n",
    "    obs = env.reset()\n",
    "\n",
    "    ep_data = {'act': [], 'obs': [], 'rews': [], 'info': [], 'all_dist': []}\n",
    "    total_reward = 0\n",
    "    num_steps = 0\n",
    "    \n",
    "    total_num_steps = 800\n",
    "    for _ in range(total_num_steps):\n",
    "        actions = agent.get_action(obs['obs'], is_deterministic=True)\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        #print(f'Step {num_steps}: obs={obs[\"obs\"]}, rews={reward}, dones={done}, info={info} \\n')\n",
    "        if store_all_agents:\n",
    "            ep_data['act'].append(actions.cpu().numpy())\n",
    "            ep_data['obs'].append(obs['obs']['state'].cpu().numpy())\n",
    "            ep_data['rews'].append(reward.cpu().numpy())  \n",
    "        else:\n",
    "            ep_data['act'].append(actions[0].cpu().numpy())\n",
    "            ep_data['obs'].append(obs['obs']['state'][0].cpu().numpy())\n",
    "            ep_data['rews'].append(reward[0].cpu().numpy())\n",
    "        #ep_data['info'].append(info)\n",
    "        x_pos = obs['obs']['state'][:,6].cpu().numpy()\n",
    "        y_pos = obs['obs']['state'][:,7].cpu().numpy()\n",
    "        ep_data['all_dist'].append(np.linalg.norm(np.array([x_pos, y_pos]), axis=0))\n",
    "        total_reward += reward[0]\n",
    "        num_steps += 1\n",
    "        is_done = done.any()\n",
    "    ep_data['obs'] = np.array(ep_data['obs'])\n",
    "    ep_data['act'] = np.array(ep_data['act'])\n",
    "    ep_data['rews'] = np.array(ep_data['rews'])\n",
    "    ep_data['all_dist'] = np.array(ep_data['all_dist'])\n",
    "\n",
    "    print(f'\\n Episode: rew_sum={total_reward:.2f}, tot_steps={num_steps} \\n')\n",
    "    #print(f'Episode data: {ep_data} \\n')\n",
    "    print(f'Episode data obs shape: {ep_data[\"obs\"].shape} \\n')\n",
    "\n",
    "    #if not cfg.headless:\n",
    "    #plot_episode_data_virtual(ep_data, evaluation_dir, store_all_agents)\n",
    "    success_rate = success_rate_from_distances(ep_data['all_dist'])\n",
    "    print(success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actions_num': [2, 2, 2, 2, 2, 2, 2, 2], 'input_shape': {'masks': (8,), 'state': (10,), 'transforms': (8, 5)}, 'num_seqs': 1, 'value_size': 1, 'normalize_value': True, 'normalize_input': True, 'normalize_input_keys': ['state']}\n",
      "['state']\n",
      "True\n",
      "10\n",
      "build mlp: 10\n",
      "RunningMeanStd:  (1,)\n",
      "['state']\n",
      "here?\n",
      "RunningMeanStd:  (10,)\n",
      "=> loading checkpoint '../corl_runs/MLP_GTXY_UF_0.25_ST_PE_0.01_PAV_1.0_PLV_0.01/nn/last_MLP_GTXY_UF_0.25_ST_PE_0.01_PAV_1.0_PLV_0.01_ep_2000_rew_589.91455.pth'\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# setting up the Isaac Gym environment and player\n",
    "\n",
    "@hydra.main(config_name=\"config\", config_path=\"../cfg\")\n",
    "def parse_hydra_configs(cfg: DictConfig):\n",
    "    # _____Set up task_____\n",
    "    horizon = 500\n",
    "    cfg.task.env.maxEpisodeLength = horizon + 2\n",
    "    cfg.task.env.platform.core.mass = 5.32\n",
    "    cfg.task.env.clipObservations['state'] = 20.0\n",
    "    cfg.task.env.task_parameters['max_spawn_dist'] = 3.0\n",
    "    cfg.task.env.task_parameters['min_spawn_dist'] = 1.5  \n",
    "    cfg.task.env.task_parameters['kill_dist'] = 6.0\n",
    "    cfg.task.env.task_parameters['kill_after_n_steps_in_tolerance'] = 800\n",
    "    cfg_dict = omegaconf_to_dict(cfg)\n",
    "    # _____Create environment_____\n",
    "    headless = cfg.headless\n",
    "    enable_viewport = \"enable_cameras\" in cfg.task.sim and cfg.task.sim.enable_cameras\n",
    "    env = VecEnvRLGames(headless=headless, sim_device=cfg.device_id, enable_livestream=cfg.enable_livestream, enable_viewport=enable_viewport)\n",
    "    \n",
    "    from omni.isaac.core.utils.torch.maths import set_seed\n",
    "    cfg.seed = set_seed(cfg.seed, torch_deterministic=cfg.torch_deterministic)\n",
    "    cfg_dict['seed'] = cfg.seed\n",
    "    task = initialize_task(cfg_dict, env)\n",
    "    rlg_trainer = RLGTrainer(cfg, cfg_dict)\n",
    "    rlg_trainer.launch_rlg_hydra(env)\n",
    "    rlg_config_dict = omegaconf_to_dict(cfg.train)\n",
    "\n",
    "    # _____Create players (model)_____\n",
    "    runner = Runner(RLGPUAlgoObserver())\n",
    "    runner.load(rlg_config_dict)\n",
    "    runner.reset()\n",
    "\n",
    "    agent = runner.create_player()\n",
    "\n",
    "    #eval_single_agent(cfg_dict, cfg, env)\n",
    "    eval_multi_agents(agent, models, horizon)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_hydra_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for inference only Player\n",
    "\n",
    "config_name = \"../cfg/train/MFP2D_PPOmulti_dict_MLP.yaml\"\n",
    "with open(config_name, 'r') as stream:\n",
    "    cfg = yaml.safe_load(stream)\n",
    "obs_space = spaces.Dict({\"state\":spaces.Box(np.ones(10) * -np.Inf, np.ones(10) * np.Inf),\n",
    "                         \"transforms\":spaces.Box(low=-1, high=1, shape=(8, 5)),\n",
    "                         \"masks\":spaces.Box(low=0, high=1, shape=(8,))})\n",
    "act_space = spaces.Tuple([spaces.Discrete(2)]*8)\n",
    "player = BasicPpoPlayerDiscrete(cfg, obs_space, act_space, clip_actions=False, deterministic=True)\n",
    "model_path = \"../corl_runs/MLP_GTXY_UF_0.25_ST_PE_0.01_PAV_1.0_PLV_0.01/nn/last_MLP_GTXY_UF_0.25_ST_PE_0.01_PAV_1.0_PLV_0.01_ep_2000_rew_589.91455.pth\"\n",
    "player.restore(model_path)\n",
    "\n",
    "obs = dict({'state':torch.zeros((1,10), dtype=torch.float32, device='cuda'),\n",
    "            'transforms': torch.zeros(5,8, device='cuda'),\n",
    "            'masks': torch.zeros(8, dtype=torch.float32, device='cuda')})\n",
    "\n",
    "action = player.get_action(obs, is_deterministic=True)\n",
    "\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
